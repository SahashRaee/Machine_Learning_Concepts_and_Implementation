# Implementation of Machine Learning Algorithm from Scratch
Learning Machine Learning from basic to advance and develop Machine Learning Models from Scratch in Pythons
sssss

## Navigation
* [Useful Commands](#useful-commands) 
* [Installation](#installation)
* [Reality vs Expectation](#reality-vs-expectation)
* [Machine Learning from Beginner to Advanced](#machine-learning-from-beginner-to-advanced)
* [Scratch Implementation](#scratch-implementation)
* [Mathematical Implementation](#mathematical-implementation)
* [Machine Learning Interview Questions with Answers](#machine-learning-interview-questions-with-answers)
* [Essential Machine Learning Formulas](#essential-machine-learning-formulas)
* [Pratice Guide for Data Science Learning](#pratice-guide-for-data-science-learning)

# Useful Resources
| Title | Repository |
|------ | :----------: |
| USEFUL GIT COMMANDS FOR EVERYDAY USE | [ðŸ”—](https://github.com/ghimiresunil/Git-Cheat-Sheet)|
| MOST USEFUL LINUX COMMANDS EVERYONE SHOULD KNOW | [ðŸ”—](https://github.com/ghimiresunil/Linux-Guide-For-All)|
| AWESOME ML TOOLBOX| [ðŸ”—](https://github.com/ghimiresunil/Awesome-ML-Toolbox)|

# Installation
| Title | Repository |
|------ | :----------: |
|INSTALL THE ANACONDA PYTHON ON WINDOWS AND LINUX | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Installation/install_anaconda_python.md)|

# Reality vs Expectation
| Title | Repository |
|------ | :----------: |
| IS AI OVERHYPED? REALITY VS EXPECTATION |[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Reality%20vs%20Expectation/Is%20AI%20Overhyped%3F%20Reality%20vs%20Expectation.md)|

# Machine Learning from Beginner to Advanced
| Title | Repository |
|------ | :----------: |
|HISTORY OF MATHEMATICS, AI & ML - HISTORY & MOTIVATION| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/mathematics_ai_ml_history_motivation.md)|
| INTRODUCTION TO ARTIFICIAL INTELLIGENCE & MACHINE LEARNING |[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Introduction%20to%20ML%20and%20AI.md)|
| KEY TERMS USED IN MACHINE LEARNING | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Key%20terms%20used%20in%20ML.md) |
|PERFORMANCE METRICS IN MACHINE LEARNING CLASSIFICATION MODEL| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Classification%20Performance%20Metrics.md) |
|PERFORMANCE METRICS IN MACHINE LEARNING REGRESSION MODEL| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Regression%20Performance%20Metrics.md) |

# Scratch Implementation
| Title | Repository |
|------ | :----------: |
|LINEAR REGRESSION FROM SCRATCH| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Linear%20Regression)|
|LOGISTIC REGRESSION FROM SCRATCH| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Logistic%20Regression)|
|NAIVE BAYES FROM SCRATCH| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Naive%20Bayes)|
|DECISION TREE FROM SCRATCH|[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/ML%20from%20Scratch/Decision%20Tree/README.md)|
|RANDOM FOREST FROM SCRATCH|[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Random%20Forest)|
| K NEAREST NEIGHBOUR | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/KNN)|
| NAIVE BAYES | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Naive%20Bayes)|
| K MEANS CLUSTERING | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/K%20Means%20Clustering)|

# Mathematical Implementation
| Title | Repository |
|------ | :----------: |
|CONFUSION MATRIX FOR YOUR MULTI-CLASS ML MODEL| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Mathematical%20Implementation/confusion_matrix.md)|

# Machine Learning Interview Questions with Answers
| Title | Repository |
|------ | :----------: |
|50 QUESTIONS ON STATISTICS & MACHINE LEARNING â€“ CAN YOU ANSWER? | [ðŸ”—](https://graspcoding.com/50-questions-on-statistics-machine-learning-can-you-answer/)|

# Essential Machine Learning Formulas
| Title | Repository |
|------ | :----------: |
|MOSTLY USED MACHINE LEARNING FORMULAS |[ðŸ”—](https://github.com/ghimiresunil/Machine-Learning-Formulas)|

# Pratice Guide for Data Science Learning
| Title | Repository |
|------ | :----------: |
| Research Guide for FYP | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Pratice%20Guide/research_guide_for_fyp.md)|
|The Intermediate Guide to 180 Days Data Science Learning Plan|[ðŸ”—](https://graspcoding.com/the-intermediate-guide-to-180-days-data-science-learning-plan/)|

*** 

### Algorithm Pros and Cons

- KN Neighbors \
   âœ” Simple, No training, No assumption about data, Easy to implement, New data can be added seamlessly, Only one hyperparameter \
   âœ– Doesn't work well in high dimensions, Sensitive to noisy data, missing values and outliers, Doesn't work well with large data sets â€”  cost of calculating distance is high, Needs feature scaling, Doesn't work well on imbalanced data, Doesn't deal well with missing values

- Decision Tree \
   âœ” Doesn't require standardization or normalization, Easy to implement, Can handle missing values, Automatic feature selection \
   âœ– High variance, Higher training time, Can become complex, Can easily overfit

- Random Forest \
   âœ” Left-out data can be used for testing, High accuracy, Provides feature importance estimates, Can handle missing values, Doesn't require feature scaling, Good performance on imbalanced datasets, Can handle large dataset, Outliers have little impact, Less overfitting \
   âœ– Less interpretable, More computational resources, Prediction time high

- Linear Regression \
   âœ” Simple, Interpretable, Easy to Implement \
   âœ– Assumes linear relationship between features, Sensitive to outliers

- Logistic Regression \
   âœ” Doesnâ€™t assume linear relationship between independent and dependent variables, Output can be interpreted as probability, Robust to noise \
   âœ– Requires more data, Effective when linearly separable

- Lasso Regression (L1) \
   âœ” Prevents overfitting, Selects features by shrinking coefficients to zero \
   âœ– Selected features will be biased, Prediction can be worse than Ridge

- Ridge Regression (L2) \
   âœ” Prevents overfitting \
   âœ– Increases bias, Less interpretability 

- AdaBoost \
   âœ” Fast, Reduced bias, Little need to tune \
   âœ– Vulnerable to noise, Can overfit

- Gradient Boosting \
   âœ” Good performance \
   âœ– Harder to tune hyperparameters

- XGBoost \
   âœ” Less feature engineering required, Outliers have little impact, Can output feature importance, Handles large datasets, Good model performance, Less prone to overfitting \â€‹
   âœ– Difficult to interpret, Harder to tune as there are numerous hyperparameters

- SVM \
   âœ” Performs well in higher dimensions, Excellent when classes are separable, Outliers have less impact \
   âœ– Slow, Poor performance with overlapping classes, Selecting appropriate kernel functions can be tricky

- NaÃ¯ve Bayes \
   âœ” Fast, Simple, Requires less training data, Scalable, Insensitive to irrelevant features, Good performance with high-dimensional data \
   âœ– Assumes independence of features

- Deep Learning \
  âœ” Superb performance with unstructured data (images, video, audio, text) \
  âœ– (Very) long training time, Many hyperparameters, Prone to overfitting


***	

***
### AI/ML dataset

| Source | Link |
|------ | :----------: |
| Google Dataset Search â€“ A search engine for datasets: | [ðŸ”—](https://datasetsearch.research.google.com/) |
| IBMâ€™s collection of datasets for enterprise applications | [ðŸ”—](https://developer.ibm.com/exchanges/data/ ) |
| Kaggle Datasets | [ðŸ”—](https://www.kaggle.com/datasets) |
| Huggingface Datasets â€“ A Python library for loading NLP datasets | [ðŸ”—](https://github.com/huggingface/datasets) |
| A large list organized by application domain | [ðŸ”—](https://github.com/awesomedata/awesome-public-datasets) |
| Computer Vision Datasets (a really large list) | [ðŸ”—](https://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm) |
| Datasetlist â€“ Datasets by domain | [ðŸ”—](https://www.datasetlist.com/) |
| OpenML â€“ A search engine for curated datasets and workflows| [ðŸ”—](https://www.openml.org/search?type=data ) |
| Papers with Code â€“ Datasets with benchmarks | [ðŸ”—](https://www.paperswithcode.com/datasets) |
| Penn Machine Learning Benchmarks | [ðŸ”—](https://github.com/EpistasisLab/pmlb/tree/master/datasets) |
| VisualDataDiscovery (for Computer Vision) | [ðŸ”—](https://www.visualdata.io/discovery) |
| UCI Machine Learning Repository | [ðŸ”—](https://archive.ics.uci.edu/ml/index.php) |
| Roboflow Public Datasets for computer vision | [ðŸ”—](https://public.roboflow.com/) |
***
